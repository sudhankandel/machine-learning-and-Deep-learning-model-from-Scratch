{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "def getMnistData(reshaped=True):\n",
    "    (train_image,train_label),(test_image,test_labels)=mnist.load_data()\n",
    "    return train_image,train_label,test_image,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MaxPool2(object):\n",
    "    def iterate_regions(self, image):\n",
    "        h, w, _ = image.shape\n",
    "\n",
    "        new_h = h // 2\n",
    "        new_w = w // 2\n",
    "\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[(i*2):(i*2 + 2), (j*2):(j * 2 + 2)]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        h, w, num_filters = input.shape\n",
    "        output = np.zeros(shape=(h // 2, w // 2, num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backprop(self, dL_dout):\n",
    "        dL_dinput = np.zeros(shape=self.last_input.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "\n",
    "            h, w, f = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        if im_region[i2, j2, f2] == amax[f2]:\n",
    "                            dL_dinput[i*2 + i2, j*2 + j2,\n",
    "                                      f2] = dL_dout[i, j, f2]\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Conv3x3(object):\n",
    "    def __init__(self, num_filters):\n",
    "        self.num_filters = num_filters\n",
    "        self.filters = np.random.randn(num_filters, 3, 3, ) / 9\n",
    "\n",
    "    def iterate_regions(self, image):\n",
    "        h, w = image.shape\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                im_region = image[i:i+3, j:j+3]\n",
    "                yield im_region, i, j\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "\n",
    "        h, w = input.shape\n",
    "\n",
    "        output = np.zeros(shape=(h-2, w-2, self.num_filters))\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input):\n",
    "            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "        return output\n",
    "\n",
    "    def backprop(self, dL_dout, learning_rate):\n",
    "        dL_dfilters = np.zeros(self.filters.shape)\n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "            for f in range(self.num_filters):\n",
    "                dL_dfilters[f] += dL_dout[i, j, f] * im_region\n",
    "        self.filters -= learning_rate * dL_dfilters\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Softmax:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(shape=nodes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        input = input.flatten()\n",
    "        self.last_input = input\n",
    "        totals = np.dot(input, self.weights) + self.biases\n",
    "        self.last_totals = totals\n",
    "        exp = np.exp(totals)\n",
    "\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def backprop(self, dL_dout, learning_rate):\n",
    "        for i, gradient in enumerate(dL_dout):\n",
    "            if gradient == 0:\n",
    "                continue\n",
    "            t_exp = np.exp(self.last_totals)\n",
    "            S = np.sum(t_exp)\n",
    "            dout_dt = -t_exp[i] * t_exp / S**2\n",
    "            dout_dt[i] = t_exp[i] * (S - t_exp[i]) / S**2\n",
    "\n",
    "            dt_dw = self.last_input\n",
    "            dt_db = 1\n",
    "            dt_dinputs = self.weights\n",
    "\n",
    "            dL_dt = gradient * dout_dt\n",
    "\n",
    "            dL_dw = dt_dw[np.newaxis].T @ dL_dt[np.newaxis]\n",
    "            dL_db = dL_dt * dt_db\n",
    "            dL_dinputs = dt_dinputs @ dL_dt\n",
    "\n",
    "            self.weights -= learning_rate * dL_dw\n",
    "            self.biases -= learning_rate * dL_db\n",
    "\n",
    "            return dL_dinputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized!\n",
      "-------Epoch 1-------\n",
      "[Step 99] Past 100 steps: Average Loss 2.218 | Accuracy: 20\n",
      "[Step 199] Past 100 steps: Average Loss 1.980 | Accuracy: 37\n",
      "[Step 299] Past 100 steps: Average Loss 1.653 | Accuracy: 42\n",
      "[Step 399] Past 100 steps: Average Loss 1.158 | Accuracy: 66\n",
      "[Step 499] Past 100 steps: Average Loss 0.912 | Accuracy: 69\n",
      "[Step 599] Past 100 steps: Average Loss 0.965 | Accuracy: 73\n",
      "[Step 699] Past 100 steps: Average Loss 0.803 | Accuracy: 70\n",
      "[Step 799] Past 100 steps: Average Loss 0.813 | Accuracy: 77\n",
      "[Step 899] Past 100 steps: Average Loss 0.474 | Accuracy: 88\n",
      "[Step 999] Past 100 steps: Average Loss 0.761 | Accuracy: 76\n",
      "[Step 1099] Past 100 steps: Average Loss 0.580 | Accuracy: 81\n",
      "[Step 1199] Past 100 steps: Average Loss 0.612 | Accuracy: 83\n",
      "[Step 1299] Past 100 steps: Average Loss 0.347 | Accuracy: 88\n",
      "[Step 1399] Past 100 steps: Average Loss 0.565 | Accuracy: 84\n",
      "[Step 1499] Past 100 steps: Average Loss 0.479 | Accuracy: 85\n",
      "[Step 1599] Past 100 steps: Average Loss 0.514 | Accuracy: 82\n",
      "[Step 1699] Past 100 steps: Average Loss 0.567 | Accuracy: 85\n",
      "[Step 1799] Past 100 steps: Average Loss 0.499 | Accuracy: 83\n",
      "[Step 1899] Past 100 steps: Average Loss 0.568 | Accuracy: 82\n",
      "[Step 1999] Past 100 steps: Average Loss 0.330 | Accuracy: 89\n",
      "[Step 2099] Past 100 steps: Average Loss 0.370 | Accuracy: 87\n",
      "[Step 2199] Past 100 steps: Average Loss 0.442 | Accuracy: 87\n",
      "[Step 2299] Past 100 steps: Average Loss 0.632 | Accuracy: 79\n",
      "[Step 2399] Past 100 steps: Average Loss 0.383 | Accuracy: 89\n",
      "[Step 2499] Past 100 steps: Average Loss 0.500 | Accuracy: 86\n",
      "[Step 2599] Past 100 steps: Average Loss 0.531 | Accuracy: 83\n",
      "[Step 2699] Past 100 steps: Average Loss 0.375 | Accuracy: 89\n",
      "[Step 2799] Past 100 steps: Average Loss 0.398 | Accuracy: 87\n",
      "[Step 2899] Past 100 steps: Average Loss 0.462 | Accuracy: 84\n",
      "[Step 2999] Past 100 steps: Average Loss 0.329 | Accuracy: 89\n",
      "-------Epoch 2-------\n",
      "[Step 99] Past 100 steps: Average Loss 0.333 | Accuracy: 88\n",
      "[Step 199] Past 100 steps: Average Loss 0.413 | Accuracy: 82\n",
      "[Step 299] Past 100 steps: Average Loss 0.439 | Accuracy: 86\n",
      "[Step 399] Past 100 steps: Average Loss 0.356 | Accuracy: 88\n",
      "[Step 499] Past 100 steps: Average Loss 0.356 | Accuracy: 86\n",
      "[Step 599] Past 100 steps: Average Loss 0.279 | Accuracy: 92\n",
      "[Step 699] Past 100 steps: Average Loss 0.309 | Accuracy: 90\n",
      "[Step 799] Past 100 steps: Average Loss 0.466 | Accuracy: 84\n",
      "[Step 899] Past 100 steps: Average Loss 0.437 | Accuracy: 86\n",
      "[Step 999] Past 100 steps: Average Loss 0.421 | Accuracy: 88\n",
      "[Step 1099] Past 100 steps: Average Loss 0.318 | Accuracy: 91\n",
      "[Step 1199] Past 100 steps: Average Loss 0.226 | Accuracy: 93\n",
      "[Step 1299] Past 100 steps: Average Loss 0.196 | Accuracy: 92\n",
      "[Step 1399] Past 100 steps: Average Loss 0.341 | Accuracy: 88\n",
      "[Step 1499] Past 100 steps: Average Loss 0.705 | Accuracy: 84\n",
      "[Step 1599] Past 100 steps: Average Loss 0.381 | Accuracy: 89\n",
      "[Step 1699] Past 100 steps: Average Loss 0.504 | Accuracy: 86\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5c31c3de027c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mnum_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnum_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5c31c3de027c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(im, label, lr)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpooling_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9fec576e95db>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, dL_dout, learning_rate)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mim_region\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_regions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_filters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mdL_dfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdL_dout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mim_region\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdL_dfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_images, train_labels, test_images, test_labels = getMnistData(\n",
    "    reshaped=False)\n",
    "train_images = train_images[:3000]\n",
    "train_labels = train_labels[:3000]\n",
    "test_images = test_images[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "conv_layer = Conv3x3(num_filters=8)       \n",
    "pooling_layer = MaxPool2()                \n",
    "softmax_layer = Softmax(13*13*8, 10)      \n",
    "\n",
    "\n",
    "def forward(image, label):\n",
    "    out = conv_layer.forward((image / 255) - 0.5)\n",
    "    out = pooling_layer.forward(out)\n",
    "    out = softmax_layer.forward(out)\n",
    "\n",
    "    loss = -np.log(out[label])\n",
    "    acc = 1 if np.argmax(out) == label else 0\n",
    "    return out, loss, acc\n",
    "\n",
    "\n",
    "def train(im, label, lr=.005):\n",
    "    out, loss, acc = forward(im, label)\n",
    "    gradient = np.zeros(10)\n",
    "    gradient[label] = -1 / out[label]\n",
    "    gradient = softmax_layer.backprop(gradient, lr)\n",
    "    gradient = pooling_layer.backprop(gradient)\n",
    "    conv_layer.backprop(gradient, lr)\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "print('MNIST CNN initialized!')\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"-------Epoch {epoch+1}-------\")\n",
    "    permutation = np.random.permutation(len(train_images))\n",
    "    train_images = train_images[permutation]\n",
    "    train_labels = train_labels[permutation]\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "        if i > 0 and i % 100 == 99:\n",
    "            print(\n",
    "                f'[Step {i}] Past 100 steps: Average Loss {loss/100:.3f} | Accuracy: {num_correct}')\n",
    "\n",
    "            loss = 0\n",
    "            num_correct = 0\n",
    "\n",
    "        l, acc = train(im, label)\n",
    "        loss += l\n",
    "        num_correct += acc\n",
    "print('\\n--- Testing the CNN ---')\n",
    "\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for im, label in zip(test_images, test_labels):\n",
    "    _, l, acc = forward(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "num_test = len(test_labels)\n",
    "print(f'Test Loss: {loss / num_test}')\n",
    "print(f'Test Accuracy: {num_correct / num_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
